{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3e8fa",
   "metadata": {},
   "source": [
    "### 1) Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa933ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adni_num = \"2\"\n",
    "experiment_type = \"txt-only_XTEST\"\n",
    "folds_num = 5\n",
    "epochs = 200\n",
    "\n",
    "\n",
    "all_columns = ['AGE','PTGENDER','ADAS11', 'MMSE', 'FAQ', \\\n",
    "               'RAVLT_immediate', 'RAVLT_learning', 'RAVLT_forgetting', \\\n",
    "               'CDRSB', 'APOE4']\n",
    "\n",
    "required_columns = ['AGE','PTGENDER','APOE4']\n",
    "experiment_name = ('_'.join(required_columns)).lower()\n",
    "print(f\"{experiment_name}\")\n",
    "\n",
    "myseed = 1\n",
    "torch.manual_seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "\n",
    "num_classes=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ea0ea",
   "metadata": {},
   "source": [
    "#### Retrieve ADNI table, with normalized values (ADNI_ready.csv)\n",
    "ADNI_ready.csv must be created manually after downloading the preferred set of subjects from the ADNI website. ADNI_ready must include the following fields:\n",
    "- subject_id\n",
    "- ADNI type (1, 2 or 3 depending to which dataset it belongs to)\n",
    "- Labels \n",
    "- 'AGE','PTGENDER','APOE4' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9b90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"ADNI_csv/\"\n",
    "filename = \"ADNI_ready.csv\"\n",
    "\n",
    "adni_tabular = pd.read_csv(os.path.join(data_path, filename))\n",
    "adni_tabular.head()\n",
    "\n",
    "# Print \n",
    "print(f\"ALL adni has {len(adni_tabular)} entries\")\n",
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"\\n {adni_tabular['labels'].value_counts()}\")\n",
    "\n",
    "if (adni_num == '123'):\n",
    "    experiment_name = 'full10'\n",
    "else:\n",
    "    adni_tabular=adni_tabular[adni_tabular['SRC']==f\"ADNI{adni_num}\"]\n",
    "\n",
    "print(f\"Adni{adni_num} has {len(adni_tabular)} entries\")\n",
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"Final:\\n {adni_tabular['labels'].value_counts()}\")\n",
    "\n",
    "\n",
    "## Check for duplicated rows\n",
    "dup = adni_tabular[adni_tabular.duplicated()]\n",
    "if not dup.empty:\n",
    "    print(f\"WARNING: Dataframe contains duplicated rows!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203d2ff",
   "metadata": {},
   "source": [
    "### 2) Dataset creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtDataset(Dataset):\n",
    "    \"\"\"Tabular and Image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, adni_df, required_columns=required_columns):\n",
    "        self.adni = adni_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.adni)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        line = self.adni.iloc[idx, 0:]\n",
    "        # Get Label\n",
    "        y = line['labels']\n",
    "\n",
    "        # Get tabular\n",
    "        tabular = line[required_columns] #line[['AGE','PTGENDER','APOE4']]\n",
    "        tabular = torch.FloatTensor(tabular)\n",
    "\n",
    "        return tabular, y\n",
    "\n",
    "img_data = TxtDataset(adni_df=adni_tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309420d",
   "metadata": {},
   "source": [
    "### Detach test set and use remaining data for train-val k-fold split (further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce539489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = adni_tabular['labels'].tolist()\n",
    "# Split data into train+val and test set indexes\n",
    "tv_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.1,shuffle=True,stratify=labels)\n",
    "\n",
    "# Create train+val dataframe and show class balance\n",
    "adni_tv = adni_tabular.iloc[tv_idx]\n",
    "print(adni_tv.groupby([\"labels\"]).count())\n",
    "tv_data = TxtDataset(adni_df=adni_tv)\n",
    "\n",
    "# Create test dataframe and show class balance\n",
    "adni_test = adni_tabular.iloc[test_idx]\n",
    "print(adni_test.groupby([\"labels\"]).count())\n",
    "test_data = TxtDataset(adni_df=adni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If executed, this cell will save the test set for this specific adni_num.\n",
    "# the saved test set can be then be shared to perform a cross-test (X-TEST) models evaluation\n",
    "if False:\n",
    "    torch.save(test_data, f'test_adni{adni_num}.pt')\n",
    "    saved_test = torch.load(f'test_adni{adni_num}.pt')\n",
    "\n",
    "    i = 10\n",
    "    print(f\"tabular = {saved_test[i][0]}, label = {saved_test[i][1]}\")\n",
    "\n",
    "    len(saved_test[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70de888",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "print(f\"tabular = {test_data[i][0]}, label = {test_data[i][1]}\")\n",
    "\n",
    "len(test_data[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bacb7a",
   "metadata": {},
   "source": [
    "### 3. Model: tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNN(nn.Module):\n",
    "\n",
    "    #Constructor\n",
    "    def __init__(self, num_variables):\n",
    "    # Call parent contructor\n",
    "        super().__init__()\n",
    "        torch.manual_seed(myseed)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln1 = nn.Linear(num_variables, 50) #num_variables sono le colonne in input\n",
    "        self.ln2 = nn.Linear(50, 50)\n",
    "        self.ln3 = nn.Linear(50, 10)\n",
    "        self.ln4 = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, tab):\n",
    "        tab = self.ln1(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln2(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln3(tab)\n",
    "        tab = self.relu(tab)\n",
    "        tab = self.ln4(tab)\n",
    "\n",
    "        return tab\n",
    "\n",
    "model = TextNN(len(test_data[i][0])) # required_columns - label column\n",
    "print(model)\n",
    "\n",
    "print('Total Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters()]))\n",
    "print('Trainable Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loaders, optimizer, criterion, epochs=500, dev='cpu', save_param = True, model_name=\"adni_only-text\"):\n",
    "    torch.manual_seed(myseed)\n",
    "    try:\n",
    "        net = net.to(dev)\n",
    "        #print(net)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Store the best val accuracy\n",
    "        best_val_accuracy = 0\n",
    "\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                if split == \"train\":\n",
    "                    net.train()\n",
    "                else:\n",
    "                    net.eval()\n",
    "                # Process each batch\n",
    "                for (tabular, labels) in loaders[split]:\n",
    "                    # Move to CUDA\n",
    "                    tabular = tabular.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = net(tabular)\n",
    "                    #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    labels = labels.float()\n",
    "                    loss = criterion(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    #pred_labels = pred.argmax(1) + 1\n",
    "                    pred_labels = (pred >= 0.0).long() # Binarize predictions to 0 and 1\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/tabular.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "                scheduler.step()\n",
    "            # Compute epoch loss/accuracy\n",
    "\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}          \n",
    "                       \n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "  \n",
    "            writer.add_scalar(\"Train Loss\", epoch_loss['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Loss\", epoch_loss['val'], epoch)\n",
    "            writer.add_scalar(\"Test Loss\", epoch_loss['test'], epoch)\n",
    "            writer.add_scalar(\"Train Accuracy\", epoch_accuracy['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Accuracy\", epoch_accuracy['val'], epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", epoch_accuracy['test'], epoch)\n",
    "\n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\",\n",
    "                  f\"LR={optimizer.param_groups[0]['lr']:.5f},\")          \n",
    "\n",
    "            \n",
    "            \n",
    "            # Store params at the best validation accuracy\n",
    "            if save_param:\n",
    "                if (epoch_accuracy['val'] > best_val_accuracy):\n",
    "                    print(f\"\\nFound new best: {epoch_accuracy['val']} - Saving best at epoch: {epoch+1}\")\n",
    "                    PATH = os.path.join(model_name,\"best_val.pth\")\n",
    "                    try:\n",
    "                        state_dict = net.module.state_dict()\n",
    "                    except AttributeError:\n",
    "                        state_dict = net.state_dict()\n",
    "                        \n",
    "                    torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'model_state_dict': state_dict,\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'loss': loss,\n",
    "                                }, PATH)\n",
    "                    best_val_accuracy = epoch_accuracy['val']\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "\n",
    "    if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set for x-tests\n",
    "outpath = f\"runs/adni{adni_num}_{experiment_type}/{experiment_name}\"\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "\n",
    "torch.save(test_data, os.path.join(outpath, f'test_adni{adni_num}.pt') )\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(myseed)\n",
    "test_loader = DataLoader(test_data,  batch_size=8, num_workers=4, drop_last=False, shuffle=False, generator=generator)\n",
    "\n",
    "tv_labels = adni_tv['labels'].tolist()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = folds_num)\n",
    "\n",
    "for fold,(train_idx,val_idx) in enumerate(skf.split(tv_data, tv_labels)):\n",
    "    \n",
    "    writer = SummaryWriter(os.path.join(outpath,f\"{fold}\"), filename_suffix=f\"_E{epochs}\")\n",
    "    print('------------fold no---------{}----------------------'.format(fold))   \n",
    "    train_df = adni_tv.iloc[train_idx]\n",
    "    train_set = TxtDataset(adni_df=train_df)\n",
    "\n",
    "    val_df = adni_tv.iloc[val_idx]\n",
    "    val_set = TxtDataset(adni_df=val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=8, num_workers=1, drop_last=False)\n",
    "    val_loader = DataLoader(val_set, batch_size=8, num_workers=1, drop_last=False)\n",
    "    \n",
    "    # Define dictionary of loaders\n",
    "    loaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader,\n",
    "               \"test\": test_loader}\n",
    "\n",
    "    # Model Params\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    # Define a loss \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.01, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    # Train model\n",
    "    train(model, loaders, optimizer, criterion, epochs=epochs, dev=dev, model_name=os.path.join(outpath,f\"{fold}\") )\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    model.apply(reset_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bfa308",
   "metadata": {},
   "source": [
    "### TEST on Cross datasets\n",
    "\n",
    "Please make sure that all the cross-test sets have been also stored in a unique directory called \"X-TEST_txt-only\" (default value). To change the default value, please update the \"x_test_dir\" variable at the beginning of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dir = 'X-TEST_txt-only'\n",
    "\n",
    "external_test_names = ['test_adni1', 'test_adni2', 'test_adni3']\n",
    "external_datal = {}\n",
    "# Load external test_sets\n",
    "for name in external_test_names:\n",
    "    \n",
    "    ext_test_path = os.path.join(outpath, x_test_dir ,f'{name}.pt')\n",
    "    loaded_test = torch.load(ext_test_path)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(loaded_test,  batch_size=8, num_workers=4, drop_last=False, shuffle=False, generator=generator)\n",
    "    external_datal[name] = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d38100",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test_results = {}\n",
    "\n",
    "for fold in list(range(folds_num)):\n",
    "    fold_results = {}\n",
    "    #saved_test = torch.load(os.path.join(outpath, f'test_adni{adni_num}.pt') )\n",
    "    best_model_path = os.path.join(outpath, f\"{fold}\",\"best_val.pth\")\n",
    "\n",
    "    model = TextNN(len(saved_test[i][0]))\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    if False:\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove module.\n",
    "            new_state_dict[name] = v\n",
    "        #load params\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    net = model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    sum_loss = {x_test:0 for x_test in external_datal }\n",
    "    sum_accuracy = {x_test:0 for x_test in external_datal }\n",
    "\n",
    "    for x_test in external_datal:\n",
    "        test_loader = external_datal[x_test]\n",
    "        for (tabular, labels) in test_loader:\n",
    "            # Move to CUDA\n",
    "            tabular = tabular.to(dev)\n",
    "            labels = labels.to(dev)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Compute output\n",
    "            pred = net(tabular)\n",
    "            #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "            labels = labels.unsqueeze(1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            # Update loss\n",
    "            sum_loss[x_test] += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            #pred_labels = pred.argmax(1) + 1\n",
    "            pred_labels = (pred >= 0.0).long() # Binarize predictions to 0 and 1\n",
    "            batch_accuracy = (pred_labels == labels).sum().item()/tabular.size(0)\n",
    "            # Update accuracy\n",
    "            sum_accuracy[x_test] += batch_accuracy\n",
    "\n",
    "        scheduler.step()\n",
    "        # Compute epoch loss/accuracy\n",
    "\n",
    "        loss = {x_test: sum_loss[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        accuracy = {x_test: sum_accuracy[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        \n",
    "        fold_results['loss'] = loss\n",
    "        fold_results['accuracy'] = accuracy\n",
    "        x_test_results[f\"{fold}\"] = fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Return results\n",
    "\n",
    "decimals = 4\n",
    "final_summary={}\n",
    "for x_test in external_datal.keys():  \n",
    "    local_summary = []\n",
    "    for f in x_test_results:\n",
    "        acc = x_test_results[f]['accuracy'][x_test]\n",
    "        local_summary.append(acc)\n",
    "        \n",
    "    final_summary[x_test] = local_summary \n",
    "    print(f\"{x_test}, \\\n",
    "          \\n Values = {local_summary}, \\\n",
    "          \\n avg = {round(np.average(local_summary), decimals)}, std = {round(np.std(local_summary),decimals)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tensorboard aftern enabling the port fwd using same port: localhost:XXXX\n",
    "#!tensorboard --logdir /PATH/TO/LOG/DIR --bind_all --load_fast=false --port=XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
