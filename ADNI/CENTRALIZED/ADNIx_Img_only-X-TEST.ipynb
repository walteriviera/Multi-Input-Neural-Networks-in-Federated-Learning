{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca7d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4f354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3e8fa",
   "metadata": {},
   "source": [
    "### 1) Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa933ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adni_num = \"3\"\n",
    "test_data_suffix = 'img-only'\n",
    "experiment_type = f\"{test_data_suffix}_XTEST\"\n",
    "folds_num = 5\n",
    "epochs = 200\n",
    "\n",
    "experiment_name = f'adni{adni_num}'\n",
    "print(f\"{experiment_name}\")\n",
    "\n",
    "myseed = 1\n",
    "torch.manual_seed(myseed)\n",
    "np.random.seed(myseed)\n",
    "\n",
    "num_classes=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66714f",
   "metadata": {},
   "source": [
    "#### Retrieve img filenames and paths\n",
    "\n",
    "The file adniX_paths.pkl is a  pikle file manually generated, containing 2 columns:\n",
    "- PTID: subject ID\n",
    "- IMG_PATH: containing the path to the T1 acquisition of the corresponding subject_ID. The path must be absolute, since it will be used by the data loader to load the specific image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the zip file name\n",
    "data_path = f\"a{adni_num}\"\n",
    "\n",
    "img_df_filename=f\"adni{adni_num}_paths.pkl\"\n",
    "filename=os.path.join(data_path, img_df_filename)\n",
    "img_df=pd.read_pickle(filename)  \n",
    "\n",
    "print(f\"Final data has {len(img_df)}\")\n",
    "img_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b5e8af",
   "metadata": {},
   "source": [
    "#### Retrieve ADNIs.csv\n",
    "\n",
    "ADNI_extracted.csv must be created manually after downloading the preferred set of subjects from the ADNI website. Each subject_id might come with various scans, meaning different types of acquisitions as well as different timing. In our study we leveraged the latest acquisition performed during first visit (oldest exam). This might vary from one case to another and since data are publicly available but only registered users can download them, we are not entitled to share our processed csv files.\n",
    "\n",
    "To replicate or simply leverage this code, the ADNI_extracted.csv must include the following fields:\n",
    "\n",
    "PTID,RefDate,RID,COLPROT,ORIGPROT,SITE,VISCODE,EXAMDATE,DX_bl,AGE,PTGENDER,PTEDUCAT,PTETHCAT,PTRACCAT,\n",
    "PTMARRY,APOE4,FDG,PIB,AV45,FBB,ABETA,TAU,PTAU,CDRSB,ADAS11,ADAS13,ADASQ4,MMSE,\n",
    "RAVLT_immediate,RAVLT_learning,RAVLT_forgetting,RAVLT_perc_forgetting,\n",
    "LDELTOTAL,DIGITSCOR,TRABSCOR,FAQ,MOCA,\n",
    "EcogPtMem,EcogPtLang,EcogPtVisspat,EcogPtPlan,EcogPtOrgan,EcogPtDivatt,EcogPtTotal,EcogSPMem,EcogSPLang,EcogSPVisspat,EcogSPPlan,EcogSPOrgan,EcogSPDivatt,EcogSPTotal,\n",
    "FLDSTRENG,FSVERSION,IMAGEUID,\n",
    "Ventricles,Hippocampus,WholeBrain,Entorhinal,Fusiform,MidTemp,\n",
    "ICV,DX,mPACCdigit,mPACCtrailsB,\n",
    "EXAMDATE_bl,CDRSB_bl,ADAS11_bl,ADAS13_bl,ADASQ4_bl,MMSE_bl,RAVLT_immediate_bl,RAVLT_learning_bl,RAVLT_forgetting_bl,\n",
    "RAVLT_perc_forgetting_bl,LDELTOTAL_BL,DIGITSCOR_bl,TRABSCOR_bl,FAQ_bl,mPACCdigit_bl,mPACCtrailsB_bl,FLDSTRENG_bl,\n",
    "FSVERSION_bl,IMAGEUID_bl,Ventricles_bl,Hippocampus_bl,WholeBrain_bl,Entorhinal_bl,Fusiform_bl,MidTemp_bl,ICV_bl,MOCA_bl,\n",
    "EcogPtMem_bl,EcogPtLang_bl,EcogPtVisspat_bl,EcogPtPlan_bl,EcogPtOrgan_bl,EcogPtDivatt_bl,EcogPtTotal_bl,\n",
    "EcogSPMem_bl,EcogSPLang_bl,EcogSPVisspat_bl,EcogSPPlan_bl,EcogSPOrgan_bl,EcogSPDivatt_bl,EcogSPTotal_bl,\n",
    "ABETA_bl,TAU_bl,PTAU_bl,FDG_bl,PIB_bl,AV45_bl,FBB_bl,\n",
    "Years_bl,Month_bl,Month,M,update_stamp,ExamDeltaToRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0ce85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"ADNI_csv\"\n",
    "filename = f\"ADNI{adni_num}_extracted.csv\"\n",
    "\n",
    "labels_col = \"DX_bl\"\n",
    "\n",
    "adni = pd.read_csv(os.path.join(data_path, filename))\n",
    "adni.head()\n",
    "\n",
    "# Print \n",
    "print(f\"Final data has {len(adni)}\")\n",
    "\n",
    "print(f\"Class distribution is organized as follow:\")\n",
    "print(f\"Final:\\n {adni[labels_col].value_counts()}\")\n",
    "adni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aaa = adni.loc[(adni['DX_bl'] == 'CN') or (adni['DX_bl'] == 'AD')]\n",
    "adni=adni.loc[adni['DX_bl'].isin(['CN','AD'])]\n",
    "adni.head()\n",
    "len(adni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c95c93",
   "metadata": {},
   "source": [
    "#### Merge csv with subj_images path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "adni_imgs = pd.merge( left=img_df, right=adni, how=\"inner\", on=\"PTID\", \n",
    "                      suffixes=(\"_x\", \"_y\"),copy=False, indicator=False, validate=\"one_to_one\")\n",
    "\n",
    "# Rename \"Acq Date column\" to \"Ref Date\" and 'Subject' to PTID to match amerge df\n",
    "adni_imgs = adni_imgs.rename(columns={'DX_bl': 'labels'})\n",
    "\n",
    "# Map labels values to numeric (CN=0, AD=1)\n",
    "adni_imgs['labels'] = np.where(adni_imgs['labels'] == \"CN\", 0, 1)\n",
    "\n",
    "# Extract only required columns for IMG_only experiments\n",
    "adni_imgs = adni_imgs[['PTID','IMG_PATH','labels']]\n",
    "\n",
    "labels_col='labels'\n",
    "print(f\"Final:\\n {adni_imgs[labels_col].value_counts()}\")\n",
    "adni_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203d2ff",
   "metadata": {},
   "source": [
    "### 2) Dataset creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ead29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    \"\"\"Tabular and Image dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, imgs_df):\n",
    "        self.images = imgs_df\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        line = self.images.iloc[idx, 0:]\n",
    "        y = line['labels']\n",
    "\n",
    "        image = nib.load(line['IMG_PATH'])\n",
    "        image = image.get_fdata() \n",
    "        #image = image[..., :3]\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.unsqueeze(dim=0)\n",
    "        \n",
    "        return image, y\n",
    "\n",
    "img_data = ImgDataset(imgs_df=adni_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d309420d",
   "metadata": {},
   "source": [
    "### Detach test set and use remaining data for train-val k-fold split (further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce539489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = adni_imgs['labels'].tolist()\n",
    "# Split data into train+val and test set indexes\n",
    "tv_idx, test_idx = train_test_split(np.arange(len(labels)), test_size=0.1,shuffle=True,stratify=labels)\n",
    "\n",
    "# Create train+val dataframe and show class balance\n",
    "adni_tv = adni_imgs.iloc[tv_idx]\n",
    "print(adni_tv.groupby([\"labels\"]).count())\n",
    "tv_data = ImgDataset(imgs_df=adni_tv)\n",
    "\n",
    "# Create test dataframe and show class balance\n",
    "adni_test = adni_imgs.iloc[test_idx]\n",
    "print(adni_test.groupby([\"labels\"]).count())\n",
    "test_data = ImgDataset(imgs_df=adni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70992330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If executed, this cell will save the test set for this specific adni_num.\n",
    "# the saved test set can be then be shared to perform a cross-test (X-TEST) models evaluation\n",
    "\n",
    "i = 10\n",
    "\n",
    "if False:\n",
    "    torch.save(test_data, f'test_adni{adni_num}_{test_data_suffix}.pt')\n",
    "    saved_test = torch.load(f'test_adni{adni_num}_{test_data_suffix}.pt')\n",
    "   \n",
    "    print(f\"{test_data[i][0].size()}, label = {test_data[i][1]}\")\n",
    "    len(saved_test[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70de888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{test_data[i][0].size()}, label = {test_data[i][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bacb7a",
   "metadata": {},
   "source": [
    "### 3. Model: img-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inplanes():\n",
    "    return [64, 128, 256, 512]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=3,\n",
    "                     stride=stride,\n",
    "                     padding=1,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "def conv1x1x1(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv3d(in_planes,\n",
    "                     out_planes,\n",
    "                     kernel_size=1,\n",
    "                     stride=stride,\n",
    "                     bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv3x3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = conv1x1x1(in_planes, planes)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = conv3x3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = conv1x1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 block_inplanes,\n",
    "                 n_input_channels=3,\n",
    "                 conv1_t_size=7,\n",
    "                 conv1_t_stride=1,\n",
    "                 no_max_pool=False,\n",
    "                 shortcut_type='B',\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=400):\n",
    "        super().__init__()\n",
    "\n",
    "        block_inplanes = [int(x * widen_factor) for x in block_inplanes]\n",
    "\n",
    "        self.in_planes = block_inplanes[0]\n",
    "        self.no_max_pool = no_max_pool\n",
    "\n",
    "        self.conv1 = nn.Conv3d(n_input_channels,\n",
    "                               self.in_planes,\n",
    "                               kernel_size=(conv1_t_size, 7, 7),\n",
    "                               stride=(conv1_t_stride, 2, 2),\n",
    "                               padding=(conv1_t_size // 2, 3, 3),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(self.in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, block_inplanes[0], layers[0],\n",
    "                                       shortcut_type)\n",
    "        self.layer2 = self._make_layer(block,\n",
    "                                       block_inplanes[1],\n",
    "                                       layers[1],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer3 = self._make_layer(block,\n",
    "                                       block_inplanes[2],\n",
    "                                       layers[2],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "        self.layer4 = self._make_layer(block,\n",
    "                                       block_inplanes[3],\n",
    "                                       layers[3],\n",
    "                                       shortcut_type,\n",
    "                                       stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(block_inplanes[3] * block.expansion, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _downsample_basic_block(self, x, planes, stride):\n",
    "        out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "        zero_pads = torch.zeros(out.size(0), planes - out.size(1), out.size(2),\n",
    "                                out.size(3), out.size(4))\n",
    "        if isinstance(out.data, torch.FloatTensor):\n",
    "            zero_pads = zero_pads\n",
    "\n",
    "        out = torch.cat([out.data, zero_pads], dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(self._downsample_basic_block,\n",
    "                                     planes=planes * block.expansion,\n",
    "                                     stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    conv1x1x1(self.in_planes, planes * block.expansion, stride),\n",
    "                    nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(in_planes=self.in_planes,\n",
    "                  planes=planes,\n",
    "                  stride=stride,\n",
    "                  downsample=downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if not self.no_max_pool:\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet18(in_channels, num_classes):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], in_channels=in_channels, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def generate_model(model_depth, **kwargs):\n",
    "    assert model_depth in [10, 18, 34, 50, 101, 152, 200]\n",
    "\n",
    "    if model_depth == 10:\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 152:\n",
    "        model = ResNet(Bottleneck, [3, 8, 36, 3], get_inplanes(), **kwargs)\n",
    "    elif model_depth == 200:\n",
    "        model = ResNet(Bottleneck, [3, 24, 36, 3], get_inplanes(), **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = generate_model(18, n_input_channels=1,\n",
    "                 widen_factor=1.0,\n",
    "                 n_classes=1)\n",
    "model = model.double()\n",
    "print(model)\n",
    "\n",
    "print('Total Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters()]))\n",
    "print('Trainable Parameters:',\n",
    "      sum([torch.numel(p) for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loaders, optimizer, criterion, epochs=500, dev='cpu', save_param = True, model_name=\"adni_only-imgs\"):\n",
    "    torch.manual_seed(myseed)\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        net = net.to(dev)\n",
    "        #print(net)\n",
    "        # Initialize history\n",
    "        history_loss = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        history_accuracy = {\"train\": [], \"val\": [], \"test\": []}\n",
    "        # Store the best val accuracy\n",
    "        best_val_accuracy = 0\n",
    "\n",
    "        # Process each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # Initialize epoch variables\n",
    "            sum_loss = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            sum_accuracy = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "            # Process each split\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                if split == \"train\":\n",
    "                    net.train()\n",
    "                else:\n",
    "                    net.eval()\n",
    "                # Process each batch\n",
    "                for (images, labels) in loaders[split]:\n",
    "                    # Move to CUDA\n",
    "                    images = images.to(dev)\n",
    "                    labels = labels.to(dev)\n",
    "                    # Reset gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # Compute output\n",
    "                    pred = net(images)\n",
    "                    #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "                    labels = labels.unsqueeze(1)\n",
    "                    labels = labels.float()\n",
    "                    loss = criterion(pred, labels)\n",
    "                    # Update loss\n",
    "                    sum_loss[split] += loss.item()\n",
    "                    # Check parameter update\n",
    "                    if split == \"train\":\n",
    "                        # Compute gradients\n",
    "                        loss.backward()\n",
    "                        # Optimize\n",
    "                        optimizer.step()\n",
    "                    # Compute accuracy\n",
    "                    #pred_labels = pred.argmax(1) + 1\n",
    "                    pred_labels = (pred >= 0).long() # Binarize predictions to 0 and 1\n",
    "                    batch_accuracy = (pred_labels == labels).sum().item()/images.size(0)\n",
    "                    # Update accuracy\n",
    "                    sum_accuracy[split] += batch_accuracy\n",
    "                scheduler.step()\n",
    "            # Compute epoch loss/accuracy\n",
    "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "            # Update history\n",
    "            for split in [\"train\", \"val\", \"test\"]:\n",
    "                history_loss[split].append(epoch_loss[split])\n",
    "                history_accuracy[split].append(epoch_accuracy[split])\n",
    "                \n",
    "            writer.add_scalar(\"Train Loss\", epoch_loss['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Loss\", epoch_loss['val'], epoch)\n",
    "            writer.add_scalar(\"Test Loss\", epoch_loss['test'], epoch)\n",
    "            writer.add_scalar(\"Train Accuracy\", epoch_accuracy['train'], epoch)\n",
    "            writer.add_scalar(\"Valid Accuracy\", epoch_accuracy['val'], epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", epoch_accuracy['test'], epoch)\n",
    "            writer.add_scalar(\"ETA\", time.time()-start_time, epoch)\n",
    "            \n",
    "            # Print info\n",
    "            print(f\"Epoch {epoch+1}:\",\n",
    "                  f\"TrL={epoch_loss['train']:.4f},\",\n",
    "                  f\"TrA={epoch_accuracy['train']:.4f},\",\n",
    "                  f\"VL={epoch_loss['val']:.4f},\",\n",
    "                  f\"VA={epoch_accuracy['val']:.4f},\",\n",
    "                  f\"TeL={epoch_loss['test']:.4f},\",\n",
    "                  f\"TeA={epoch_accuracy['test']:.4f},\",\n",
    "                  f\"LR={optimizer.param_groups[0]['lr']:.5f},\"\n",
    "                  f\"s={time.time()-start_time:.4f},\")\n",
    "\n",
    "            # Store params at the best validation accuracy\n",
    "            if save_param:\n",
    "                if (epoch_accuracy['val'] > best_val_accuracy):\n",
    "                    print(f\"\\nFound new best: {epoch_accuracy['val']} - Saving best at epoch: {epoch+1}\")\n",
    "                    PATH = os.path.join(model_name,\"best_val.pth\")\n",
    "                    try:\n",
    "                        state_dict = net.module.state_dict()\n",
    "                    except AttributeError:\n",
    "                        state_dict = net.state_dict()\n",
    "                        \n",
    "                    torch.save({\n",
    "                                'epoch': epoch,\n",
    "                                'model_state_dict': state_dict,\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'loss': loss,\n",
    "                                }, PATH)\n",
    "                    best_val_accuracy = epoch_accuracy['val']\n",
    "            \n",
    "            \n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    finally:\n",
    "        # Plot loss\n",
    "        plt.title(\"Loss\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_loss[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Plot accuracy\n",
    "        plt.title(\"Accuracy\")\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            plt.plot(history_accuracy[split], label=split)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "\n",
    "    if isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set for x-tests\n",
    "outpath = os.path.join(f\"runs\",f\"adni{adni_num}_{experiment_type}\",f\"{experiment_name}\")\n",
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(myseed)\n",
    "test_loader = DataLoader(test_data,  batch_size=8, num_workers=1, drop_last=False, shuffle=False, generator=generator)\n",
    "\n",
    "tv_labels = adni_tv['labels'].tolist()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = folds_num)\n",
    "\n",
    "for fold,(train_idx,val_idx) in enumerate(skf.split(tv_data, tv_labels)):\n",
    "    \n",
    "    writer = SummaryWriter(os.path.join(outpath,f\"{fold}\"), filename_suffix=f\"_E{epochs}\")\n",
    "    print('------------fold no---------{}----------------------'.format(fold))   \n",
    "    train_df = adni_tv.iloc[train_idx]\n",
    "    train_set = ImgDataset(imgs_df=train_df)\n",
    "\n",
    "    val_df = adni_tv.iloc[val_idx]\n",
    "    val_set = ImgDataset(imgs_df=val_df)\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=8, drop_last=False)\n",
    "    val_loader = DataLoader(val_set, batch_size=8, drop_last=False)\n",
    "    \n",
    "    # Define dictionary of loaders\n",
    "    loaders = {\"train\": train_loader,\n",
    "               \"val\": val_loader,\n",
    "               \"test\": test_loader}\n",
    "\n",
    "    # Model Params\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    # Define a loss \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr = 0.01, epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    # Train model\n",
    "    train(model, loaders, optimizer, criterion, epochs=epochs, dev=dev, model_name=os.path.join(outpath,f\"{fold}\"))\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    model.apply(reset_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a13c049",
   "metadata": {},
   "source": [
    "### TEST on Cross datasets\n",
    "\n",
    "Please make sure that all the cross-test sets have been also stored in a unique directory called \"X-TEST_img-only\" (default value). To change the default value, please update the \"x_test_dir\" variable at the beginning of the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd813",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dir = 'X-TEST_img-only'\n",
    "\n",
    "external_test_names = ['test_adni1', 'test_adni2', 'test_adni3']\n",
    "external_datal = {}\n",
    "# Load external test_sets\n",
    "for name in external_test_names:\n",
    "    \n",
    "    ext_test_path = os.path.join(outpath, x_test_dir ,f'{name}_{test_data_suffix}.pt')\n",
    "    loaded_test = torch.load(ext_test_path)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    test_loader = DataLoader(loaded_test,  batch_size=8, num_workers=4, drop_last=False, shuffle=False, generator=generator)\n",
    "    external_datal[name] = test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fdc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_results = {}\n",
    "\n",
    "for fold in list(range(folds_num)):\n",
    "#for fold in [0]:\n",
    "    fold_results = {}\n",
    "    #saved_test = torch.load(os.path.join(outpath, f'test_adni{adni_num}.pt') )\n",
    "    best_model_path = os.path.join(outpath, f\"{fold}\",\"best_val.pth\")\n",
    "\n",
    "    model = generate_model(18, n_input_channels=1, widen_factor=1.0, n_classes=1)\n",
    "    model = model.double()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    if False:\n",
    "        from collections import OrderedDict\n",
    "        new_state_dict = OrderedDict()\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:] # remove module.\n",
    "            new_state_dict[name] = v\n",
    "        #load params\n",
    "        model.load_state_dict(new_state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    net = model.eval()\n",
    "\n",
    "\n",
    "\n",
    "    sum_loss = {x_test:0 for x_test in external_datal }\n",
    "    sum_accuracy = {x_test:0 for x_test in external_datal }\n",
    "\n",
    "    for x_test in external_datal:\n",
    "        test_loader = external_datal[x_test]\n",
    "        for (tabular, labels) in test_loader:\n",
    "            # Move to CUDA\n",
    "            tabular = tabular.to(dev)\n",
    "            labels = labels.to(dev)\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Compute output\n",
    "            pred = net(tabular)\n",
    "            #pred = pred.squeeze(dim=1) # Output shape is [Batch size, 1], but we want [Batch size]\n",
    "            labels = labels.unsqueeze(1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(pred, labels)\n",
    "\n",
    "            # Update loss\n",
    "            sum_loss[x_test] += loss.item()\n",
    "\n",
    "            # Compute accuracy\n",
    "            #pred_labels = pred.argmax(1) + 1\n",
    "            pred_labels = (pred >= 0.0).long() # Binarize predictions to 0 and 1\n",
    "            batch_accuracy = (pred_labels == labels).sum().item()/tabular.size(0)\n",
    "            # Update accuracy\n",
    "            sum_accuracy[x_test] += batch_accuracy\n",
    "\n",
    "        scheduler.step()\n",
    "        # Compute epoch loss/accuracy\n",
    "\n",
    "        loss = {x_test: sum_loss[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        accuracy = {x_test: sum_accuracy[x_test]/len(external_datal[x_test]) for x_test in list(external_datal.keys())}\n",
    "        \n",
    "        fold_results['loss'] = loss\n",
    "        fold_results['accuracy'] = accuracy\n",
    "        x_test_results[f\"{fold}\"] = fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef4986",
   "metadata": {},
   "outputs": [],
   "source": [
    "decimals = 4\n",
    "final_summary={}\n",
    "print(f\"training on ADNI{adni_num}, {test_data_suffix}\")\n",
    "for x_test in external_datal.keys():  \n",
    "    local_summary = []\n",
    "    for f in x_test_results:\n",
    "        acc = x_test_results[f]['accuracy'][x_test]\n",
    "        local_summary.append(acc)\n",
    "        \n",
    "    final_summary[x_test] = local_summary \n",
    "    print(f\"{x_test}, \\\n",
    "          \\n Values = {local_summary}, \\\n",
    "          \\n avg = {round(np.average(local_summary), decimals)}, std = {round(np.std(local_summary),decimals)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the tensorboard aftern enabling the port fwd using same port: localhost:XXXX\n",
    "#!tensorboard --logdir /PATH/TO/LOG/DIR --bind_all --load_fast=false --port=XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
